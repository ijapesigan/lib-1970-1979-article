@Article{Bradley-1978,
  author = {James V. Bradley},
  date = {1978-11},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  title = {Robustness?},
  doi = {10.1111/j.2044-8317.1978.tb00581.x},
  number = {2},
  pages = {144--152},
  volume = {31},
  publisher = {Wiley},
  abstract = {The actual behaviour of the probability of a Type I error under assumption violation is quite complex, depending upon a wide variety of interacting factors. Yet allegations of robustness tend to ignore its highly particularistic nature and neglect to mention important qualifying conditions. The result is often a vast overgeneralization which nevertheless is difficult to refute since a standard quantitative definition of what constitutes robustness does not exist. Yet under any halfway reasonable quantitative definition, many of the most prevalent claims of robustness would be demonstrably false. Therefore robustness is a highly questionable concept.},
  annotation = {robustness},
}

@Article{Cronbach-Furby-1970,
  author = {Lee J. Cronbach and Lita Furby},
  date = {1970-07},
  journaltitle = {Psychological Bulletin},
  title = {How we should measure ``change'': Or should we?},
  doi = {10.1037/h0029382},
  number = {1},
  pages = {68--80},
  volume = {74},
  abstract = {Examines procedures previously recommended by various authors for the estimation of ``change'' scores, ``residual,'' or ``basefree'' measures of change, and other kinds of difference scores. A procedure proposed by F. M. Lord is extended to obtain more precise estimates, and an alternative to the L. R. Tucker, F. Damarin, and S. A. Messick (see 41:3) procedure is offered. A consideration of the purposes for which change measures have been sought in the past leads to a series of recommended procedures which solve research and personnel-decision problems without estimation of change scores for individuals.},
  publisher = {American Psychological Association ({APA})},
}

@Article{Efron-1979a,
  author = {Bradley Efron},
  date = {1979-01},
  journaltitle = {The Annals of Statistics},
  title = {Bootstrap methods: Another look at the jackknife},
  doi = {10.1214/aos/1176344552},
  number = {1},
  volume = {7},
  abstract = {We discuss the following problem: given a random sample $\mathbf{X} = \left( X_1 , X_2 , \dots , X_n \right)$ from an unknown probability distribution $F$, estimate the sampling distribution of some prespecified random variable $R \left( \mathbf{X}, F \right)$, on the basis of the observed data $\mathbf{x}$. (Standard jackknife theory gives an approximate mean and variance in the case $R \left( \mathbf{X}, F \right) = \theta \left( \hat{F} \right) - \theta \left( F \right)$, $\theta$ some parameter of interest.) A general method, called the ``bootstrap'' is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
  publisher = {Institute of Mathematical Statistics},
  keywords = {bootstrap, discriminant analysis, error rate estimation, jackknife, nonlinear regression, nonparametric variance estimation, resampling, subsample values},
}

@Article{Efron-1979b,
  author = {Bradley Efron},
  date = {1979-10},
  journaltitle = {{SIAM} Review},
  title = {Computers and the theory of statistics: Thinking the unthinkable},
  doi = {10.1137/1021092},
  number = {4},
  pages = {460--480},
  volume = {21},
  abstract = {This is a survey article concerning recent advances in certain areas of statistical theory, written for a mathematical audience with no background in statistics. The topics are chosen to illustrate a special point: how the advent of the high-speed computer has affected the development of statistical theory. The topics discussed include nonparametric methods, the jackknife, the bootstrap, cross-validation, error-rate estimation in discriminant analysis, robust estimation, the influence function, censored data, the EM algorithm, and Cox's likelihood function. The exposition is mainly by example, with only a little offered in the way of theoretical development.},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Engel-1977,
  author = {George L. Engel},
  date = {1977-04},
  journaltitle = {Science},
  title = {The need for a new medical model: A challenge for biomedicine},
  doi = {10.1126/science.847460},
  issn = {1095-9203},
  number = {4286},
  pages = {129--136},
  volume = {196},
  abstract = {The dominant model of disease today is biomedical, and it leaves no room within tis framework for the social, psychological, and behavioral dimensions of illness. A biopsychosocial model is proposed that provides a blueprint for research, a framework for teaching, and a design for action in the real world of health care. },
  publisher = {American Association for the Advancement of Science (AAAS)},
}

@Article{Hinkley-1977,
  author = {David V. Hinkley},
  date = {1977-08},
  journaltitle = {Technometrics},
  title = {Jackknifing in unbalanced situations},
  doi = {10.1080/00401706.1977.10489550},
  number = {3},
  pages = {285--292},
  volume = {19},
  abstract = {Both the standard jackknife and a weighted jackknife are investigated in the general linear model situation. Properties of bias reduction and standard error estimation are derived and the weighted jackknife shown to be superior for unbalanced data. There is a preliminary discussion of robust regression fitting using jackknife pseudo-values.},
  publisher = {Informa {UK} Limited},
  keywords = {jackknife, linear model, regression, residual, robustness,},
  annotation = {regression, regression-hc},
}

@Article{Horn-Horn-Duncan-1975,
  author = {Susan D. Horn and Roger A. Horn and David B. Duncan},
  date = {1975-06},
  journaltitle = {Journal of the American Statistical Association},
  title = {Estimating heteroscedastic variances in linear models},
  doi = {10.1080/01621459.1975.10479877},
  number = {350},
  pages = {380--385},
  volume = {70},
  publisher = {Informa {UK} Limited},
  annotation = {regression, regression-hc},
}

@Article{Nesselroade-Cable-1974,
  author = {John R. Nesselroade and Dana G. Cable},
  date = {1974-07},
  journaltitle = {Multivariate Behavioral Research},
  title = {Sometimes, it's okay to factor difference scores" - The separation of state and trait anxiety},
  doi = {10.1207/s15327906mbr0903_3},
  number = {3},
  pages = {273--284},
  volume = {9},
  abstract = {Contemporary psychometric policy and practice have tended to make the use of algebraic difference scores in psychological research taboo. Within the more limited domain of factor analytic research on personality, difference scores have been the subject of sporadic debate for more than 30 years. Using the personality trait versus state distinction as a substantive context, the fit of the factor analytic model to difference score data is investigated and found to be quite good. Methodological issues related to properties of difference scores and their implications for personality research are briefly discussed.},
  publisher = {Informa {UK} Limited},
}

@Article{Osborne-Suddick-1972,
  author = {R. T. Osborne and D. E. Suddick},
  date = {1972-09},
  journaltitle = {The Journal of Genetic Psychology},
  title = {A Longitudinal Investigation of the Intellectual Differentiation Hypothesis},
  doi = {10.1080/00221325.1972.10533131},
  issn = {1940-0896},
  number = {1},
  pages = {83--89},
  volume = {121},
  publisher = {Informa UK Limited},
}

@Article{Rubin-1976,
  author = {Donald B. Rubin},
  date = {1976},
  journaltitle = {Biometrika},
  title = {Inference and missing data},
  doi = {10.1093/biomet/63.3.581},
  number = {3},
  pages = {581--592},
  volume = {63},
  abstract = {When making sampling distribution inferences about the parameter of the data, $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about $\theta$, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from $\theta$. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  publisher = {Oxford University Press ({OUP})},
}
